{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":11828209,"sourceType":"datasetVersion","datasetId":7344699},{"sourceId":240780560,"sourceType":"kernelVersion"},{"sourceId":240787643,"sourceType":"kernelVersion"},{"sourceId":413505,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":337521,"modelId":358490}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport os\nfrom torchvision.models import efficientnet_b0\nimport timm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport time\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport utility_modele as md\nfrom sklearn.model_selection import train_test_split\nimport json\n\nimport os\nimport time\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport timm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:58:58.383837Z","iopub.execute_input":"2025-05-27T08:58:58.384280Z","iopub.status.idle":"2025-05-27T08:58:58.392155Z","shell.execute_reply.started":"2025-05-27T08:58:58.384254Z","shell.execute_reply":"2025-05-27T08:58:58.391146Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"csv_path = \"/kaggle/input/rayon-x-label/DB_patients_norm_bbox.csv\"\ndf = pd.read_csv(csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:59:01.485622Z","iopub.execute_input":"2025-05-27T08:59:01.485919Z","iopub.status.idle":"2025-05-27T08:59:02.028936Z","shell.execute_reply.started":"2025-05-27T08:59:01.485895Z","shell.execute_reply":"2025-05-27T08:59:02.028294Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def filtrer_maladie_unique(df, maladie_cible, autres_maladies):\n    \"\"\"\n    Retourne les lignes du DataFrame où :\n    - maladie_cible == 1\n    - toutes les autres maladies == 0\n    \"\"\"\n    condition_cible = df[maladie_cible] == 1\n    autres = [m for m in autres_maladies if m != maladie_cible]\n    condition_autres = df[autres].sum(axis=1) == 0\n\n    return df[condition_cible & condition_autres]\n\nmaladies = [\n    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia', 'No Finding'\n]\n\ndf_atelectasis_seule = filtrer_maladie_unique(df, \"Atelectasis\", maladies)\ndf_no_finding_seule = filtrer_maladie_unique(df, \"No Finding\", maladies)\ndf_no_finding_seule = df_no_finding_seule.sample(n=5000, random_state=42)\ndf_atelectasis_final = pd.concat([df_atelectasis_seule, df_no_finding_seule], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:59:06.351308Z","iopub.execute_input":"2025-05-27T08:59:06.351615Z","iopub.status.idle":"2025-05-27T08:59:06.419050Z","shell.execute_reply.started":"2025-05-27T08:59:06.351591Z","shell.execute_reply":"2025-05-27T08:59:06.418351Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class CLAHETransform:\n    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n\n    def __call__(self, img):\n        # Convert PIL to grayscale numpy\n        img_np = np.array(img.convert('L'))  # convert to grayscale\n        img_clahe = self.clahe.apply(img_np)\n\n        # Reconvert to PIL with 3 channels (fake RGB)\n        img_rgb = Image.fromarray(cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB))\n        return img_rgb\n\ndef detect_lung_roi(image_pil):\n    img_gray = np.array(image_pil.convert('L'))  # PIL → grayscale numpy\n    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)\n    _, thresh = cv2.threshold(img_blur, 30, 255, cv2.THRESH_BINARY)\n\n    # Trouver les contours\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not contours:\n        return image_pil  # fallback\n\n    # Récupérer le plus grand contour\n    largest = max(contours, key=cv2.contourArea)\n    x, y, w, h = cv2.boundingRect(largest)\n\n    # Crop l'image selon le ROI détecté\n    img_cropped = img_gray[y:y+h, x:x+w]\n    img_rgb = cv2.cvtColor(img_cropped, cv2.COLOR_GRAY2RGB)\n    return Image.fromarray(img_rgb)\n\nclass LungROICrop:\n    def __call__(self, img):\n        return detect_lung_roi(img)\n\nclass CenterCropZoom:\n    def __init__(self, zoom_factor=1.2):\n        self.zoom_factor = zoom_factor\n\n    def __call__(self, img):\n        w, h = img.size\n        new_w = int(w / self.zoom_factor)\n        new_h = int(h / self.zoom_factor)\n        left = (w - new_w) // 2\n        top = (h - new_h) // 2\n        right = left + new_w\n        bottom = top + new_h\n        img_cropped = img.crop((left, top, right, bottom))\n        return img_cropped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:59:27.139038Z","iopub.execute_input":"2025-05-27T08:59:27.139663Z","iopub.status.idle":"2025-05-27T08:59:27.149626Z","shell.execute_reply.started":"2025-05-27T08:59:27.139634Z","shell.execute_reply":"2025-05-27T08:59:27.148921Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_atelectasis_final[\"Finding Labels\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:14:12.181432Z","iopub.execute_input":"2025-05-26T23:14:12.181708Z","iopub.status.idle":"2025-05-26T23:14:12.217010Z","shell.execute_reply.started":"2025-05-26T23:14:12.181684Z","shell.execute_reply":"2025-05-26T23:14:12.216295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Dataset PyTorch multi-label\nclass ChestXrayDataset(Dataset):\n    def __init__(self, dataframe, label_cols, transform=None):\n        self.df = dataframe\n        self.label_cols = label_cols\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform = transform    \n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image Path']).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        labels = torch.tensor(row[self.label_cols].astype(np.float32).values, dtype=torch.float32)\n        return image, labels\n\n    def __afficher_image__(self, idx):\n        \"\"\"\n        Affiche l'image (dénormalisée) et ses labels associés.\n        \"\"\"\n        image, label = self[idx]\n\n        # # Dénormalisation\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        image = image * std + mean  # Inverser la normalisation\n\n        # Passage à la forme (H, W, C) pour matplotlib\n        image = image.permute(1, 2, 0).clamp(0, 1)\n\n        # Affichage\n        plt.imshow(image)\n        plt.title(f\"Label: {label}\")\n        plt.axis('off')\n        plt.show()\n\n        # Affichage des maladies détectées\n        nb_maladie = 0\n        for i in range(len(label)):\n            if label[i] == 1:\n                print(\"Maladies présentes :\")\n                print(f\" - {self.label_cols[i]}\")\n                nb_maladie += 1\n        if(nb_maladie == 0):\n            print(\"Aucune maladie\")\n\n# 3. Transforms\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.RandomHorizontalFlip(p=0.5),\n#     transforms.RandomRotation(10),\n#     transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n#     transforms.ColorJitter(brightness=0.2, contrast=0.2),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n# ])\n\n\ntransform = transforms.Compose([\n    LungROICrop(),\n    CLAHETransform(),\n    transforms.Resize((256, 256)),  # Agrandit\n    CenterCropZoom(zoom_factor=1.2),  # Zoom centré\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.RandomRotation(5),\n    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# 4. Split des données\n# 2. Préparation des données\nlabel_columns = ['Atelectasis']\n\ntrain_df, val_df = train_test_split(df_atelectasis_final, test_size=0.2, random_state=42)\n\n\ntrain_dataset = ChestXrayDataset(train_df, label_columns, transform)\nval_dataset = ChestXrayDataset(val_df, label_columns, transform)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\n# 5. Modèle ViT\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\nmodel = model.cuda()\n\n\nratio_neg, ratio_pos = df_atelectasis_final[\"Finding Labels\"].value_counts()\npos_weight = torch.tensor([ratio_neg/ratio_pos]).cuda()\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nbest_f1 = 0\ntrain_losses = []\nval_losses = []\n\n# 7. Validation\n\ndef evaluate(model, loader):\n    model.eval()\n    all_labels, all_outputs = [], []\n    total_loss = 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.cuda(), labels.cuda()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            outputs = torch.sigmoid(outputs).cpu().numpy()\n            all_outputs.extend(outputs)\n            all_labels.extend(labels.cpu().numpy())\n    val_loss = total_loss / len(loader)\n    auc = roc_auc_score(all_labels, all_outputs)\n    preds = [1 if o >= 0.5 else 0 for o in all_outputs]\n    f1 = f1_score(all_labels, preds)\n    precision = precision_score(all_labels, preds)\n    recall = recall_score(all_labels, preds)\n    accuracy = accuracy_score(all_labels, preds)\n    return val_loss, auc, f1, precision, recall, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:59:29.030798Z","iopub.execute_input":"2025-05-27T08:59:29.031565Z","iopub.status.idle":"2025-05-27T08:59:30.974410Z","shell.execute_reply.started":"2025-05-27T08:59:29.031536Z","shell.execute_reply":"2025-05-27T08:59:30.973600Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 8. Boucle d'entraînement\npatience = 5\nbest_f1 = 0\nepochs_no_improve = 0\nmax_epochs = 50\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(max_epochs):\n    model.train()\n    epoch_loss = 0\n    start_time = time.time()\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        images, labels = images.cuda(), labels.cuda()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n    train_losses.append(epoch_loss)\n\n    val_loss, auc, f1, precision, recall, accuracy = evaluate(model, val_loader)\n    val_losses.append(val_loss)\n    elapsed = time.time() - start_time\n\n    print(f\"Epoch {epoch+1} - Time: {elapsed:.2f}s - Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - AUC: {auc:.4f} - F1: {f1:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Accuracy: {accuracy:.4f}\")\n\n    if f1 > best_f1:\n        best_f1 = f1\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n        with open(\"best_metrics.txt\", \"w\") as f:\n            f.write(f\"Epoch: {epoch+1}\\nLoss: {epoch_loss:.4f}\\nVal Loss: {val_loss:.4f}\\nAUC: {auc:.4f}\\nF1: {f1:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nAccuracy: {accuracy:.4f}\\n\")\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(\"Early stopping activé après\", patience, \"époques sans amélioration.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:59:31.763390Z","iopub.execute_input":"2025-05-27T08:59:31.764123Z","iopub.status.idle":"2025-05-27T08:59:48.736921Z","shell.execute_reply.started":"2025-05-27T08:59:31.764059Z","shell.execute_reply":"2025-05-27T08:59:48.735582Z"}},"outputs":[{"name":"stderr","text":"Epoch 1:   3%|▎         | 7/231 [00:16<09:00,  2.41s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/782760127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2881804514.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image Path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2993707653.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLungROICrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdetect_lung_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCenterCropZoom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2993707653.py\u001b[0m in \u001b[0;36mdetect_lung_roi\u001b[0;34m(image_pil)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mimg_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_gray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mimg_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cropped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLungROICrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3175\u001b[0m             \u001b[0mdecoder_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"with open(\"/kaggle/working/best_metrics.txt\", \"r\") as f:\n    lines = f.readlines()\n    for line in lines:\n        print(line.strip())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:40:19.162218Z","iopub.execute_input":"2025-05-27T00:40:19.162949Z","iopub.status.idle":"2025-05-27T00:40:19.167417Z","shell.execute_reply.started":"2025-05-27T00:40:19.162928Z","shell.execute_reply":"2025-05-27T00:40:19.166673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Courbe des pertes durant l\\'entraînement')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:40:35.460451Z","iopub.execute_input":"2025-05-27T00:40:35.460957Z","iopub.status.idle":"2025-05-27T00:40:35.712071Z","shell.execute_reply.started":"2025-05-27T00:40:35.460934Z","shell.execute_reply":"2025-05-27T00:40:35.711335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10. Attention Rollout pour ViT\nclass VitAttentionRollout:\n    def __init__(self, model, head_fusion='mean'):\n        self.model = model\n        self.head_fusion = head_fusion\n        self.attentions = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for blk in self.model.blocks:\n            blk.attn.register_forward_hook(self._get_attention)\n\n\n    def _get_attention(self, module, input, output):\n        self.attentions.append(output.detach().cpu())\n\n    def __call__(self, input_tensor):\n        self.attentions = []\n        _ = self.model(input_tensor)\n        result = torch.eye(self.attentions[0].size(-1))\n        for attention in self.attentions:\n            if self.head_fusion == 'mean':\n                attention_heads_fused = attention.mean(dim=1)\n            else:\n                raise ValueError(\"head_fusion must be 'mean'\")\n            attention_heads_fused = attention_heads_fused + torch.eye(attention_heads_fused.size(-1))\n            attention_heads_fused = attention_heads_fused / attention_heads_fused.sum(dim=-1, keepdim=True)\n            result = torch.matmul(attention_heads_fused, result)\n        mask = result[0, 1:]\n        n_tokens = mask.shape[0]\n        side = int(n_tokens ** 0.5)\n        return mask[:side*side].reshape(side, side).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:16:16.407601Z","iopub.execute_input":"2025-05-26T22:16:16.408153Z","iopub.status.idle":"2025-05-26T22:16:16.414410Z","shell.execute_reply.started":"2025-05-26T22:16:16.408132Z","shell.execute_reply":"2025-05-26T22:16:16.413637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def __getitem__(self, idx):\n#     row = self.df.iloc[idx]\n#     image = Image.open(row['Image Path']).convert('RGB')\n#     if self.transform:\n#         image = self.transform(image)\n#     labels = torch.tensor(row[self.label_cols].astype(np.float32).values, dtype=torch.float32)\n#     return image, labels\n\nimg =  Image.open(\"/kaggle/input/data/images_004/images/00006904_007.png\").convert('RGB')\na = transform(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:58:08.209313Z","iopub.execute_input":"2025-05-24T18:58:08.210010Z","iopub.status.idle":"2025-05-24T18:58:08.240074Z","shell.execute_reply.started":"2025-05-24T18:58:08.209989Z","shell.execute_reply":"2025-05-24T18:58:08.239507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 11. Application de l'explicabilité\nmodel.eval()\nrollout = VitAttentionRollout(model)\n\nsample_image = a  # 'a' est une image transformée déjà prête (Tensor)\nsample_image_input = sample_image.unsqueeze(0).cuda()\n\n# Prédiction\nwith torch.no_grad():\n    output = model(sample_image_input)\n    prob = torch.sigmoid(output).item()\n\n# Attention rollout\nattention_mask = rollout(sample_image_input)\n\n# Visualisation\nplt.imshow(sample_image.permute(1, 2, 0).cpu().numpy().clip(0, 1))\nplt.imshow(attention_mask, cmap='jet', alpha=0.5, extent=(0, 224, 224, 0))\nplt.title(f\"Attention Rollout - ViT\\nProbabilité d'Atelectasis : {prob:.2f}\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:58:09.723087Z","iopub.execute_input":"2025-05-24T18:58:09.723743Z","iopub.status.idle":"2025-05-24T18:58:10.050271Z","shell.execute_reply.started":"2025-05-24T18:58:09.723723Z","shell.execute_reply":"2025-05-24T18:58:10.049483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_image(image_path):\n    model.eval()\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).cuda()\n    with torch.no_grad():\n        output = model(image)\n        prob = torch.sigmoid(output).item()\n    print(f\"Probabilité de présence d'Atelectasis: {prob:.4f}\")\n    if prob >= 0.5:\n        print(\"=> Maladie détectée (Atelectasis)\")\n    else:\n        print(\"=> Aucune maladie détectée\")\n\npredict_image(\"/kaggle/input/data/images_004/images/00006904_007.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:58:00.965278Z","iopub.execute_input":"2025-05-24T18:58:00.965516Z","iopub.status.idle":"2025-05-24T18:58:01.043599Z","shell.execute_reply.started":"2025-05-24T18:58:00.965501Z","shell.execute_reply":"2025-05-24T18:58:01.042917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_atelectasis_final[df_atelectasis_final[\"Atelectasis\"] == 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:26:10.195053Z","iopub.status.idle":"2025-05-24T17:26:10.195341Z","shell.execute_reply.started":"2025-05-24T17:26:10.195178Z","shell.execute_reply":"2025-05-24T17:26:10.195191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_vit = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=1)\n\n# 2. Charger les poids\nstate_dict = torch.load(\"/kaggle/working/best_model.pth\", map_location=\"cpu\")\nmodel_vit.load_state_dict(state_dict)\n\n# 3. Sauvegarder en .pt (modèle complet)\ntorch.save(model_vit, \"best_model_vit.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:51:59.085836Z","iopub.execute_input":"2025-05-24T18:51:59.086517Z","iopub.status.idle":"2025-05-24T18:52:00.936701Z","shell.execute_reply.started":"2025-05-24T18:51:59.086493Z","shell.execute_reply":"2025-05-24T18:52:00.936150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"/kaggle/working/best_model_vit.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:55:12.807044Z","iopub.execute_input":"2025-05-24T18:55:12.807294Z","iopub.status.idle":"2025-05-24T18:55:12.811965Z","shell.execute_reply.started":"2025-05-24T18:55:12.807278Z","shell.execute_reply":"2025-05-24T18:55:12.811218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_columns = [\"Atelectasis\"]\nmodel1 = efficientnet_b0(pretrained=False)\nmodel1.classifier[1] = nn.Linear(model1.classifier[1].in_features, len(label_columns))\n\n# 2. Charger les poids sur CPU\nstate_dict = torch.load(\n    \"/kaggle/working/efficientnetb0_appa_atelectasis_v3.pt\",\n    map_location=torch.device(\"cpu\")\n)\nmodel1.load_state_dict(state_dict)\n\n# 3. Mettre sur le bon device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel1 = model1.to(device)\nmodel1.eval()\n\nmd.afficher_metrics_json(\"/kaggle/working/efficientnetb0_appa_atelectasis_v3.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom timm import create_model\n\n# 1. Définir les labels\nlabel_columns = [\"Atelectasis\"]\n\n# 2. Créer le modèle ViT (ex: vit_base_patch16_224)\nmodel_vit = create_model(\n    \"vit_base_patch16_224\",     # ⚠️ à adapter si tu as utilisé une autre version\n    pretrained=False,\n    num_classes=len(label_columns)\n)\n\n# 3. Charger les poids (sur CPU)\nstate_dict = torch.load(\n    \"/kaggle/input/vit-atelectasis/pytorch/default/1/vit_at.pth\", \n    map_location=torch.device(\"cpu\")\n)\nmodel_vit.load_state_dict(state_dict)\n\n# 4. Mettre le modèle sur le bon device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_vit = model_vit.to(device)\nmodel_vit.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:03:18.574082Z","iopub.execute_input":"2025-05-26T22:03:18.574402Z","iopub.status.idle":"2025-05-26T22:03:20.722390Z","shell.execute_reply.started":"2025-05-26T22:03:18.574382Z","shell.execute_reply":"2025-05-26T22:03:20.721441Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10. Attention Rollout pour ViT\nclass VitAttentionRollout:\n    def __init__(self, model, head_fusion='mean'):\n        self.model = model\n        self.head_fusion = head_fusion\n        self.attentions = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for blk in self.model.blocks:\n            blk.attn.register_forward_hook(self._get_attention)\n\n\n    def _get_attention(self, module, input, output):\n        self.attentions.append(output.detach().cpu())\n\n    def __call__(self, input_tensor):\n        self.attentions = []\n        _ = self.model(input_tensor)\n        result = torch.eye(self.attentions[0].size(-1))\n        for attention in self.attentions:\n            if self.head_fusion == 'mean':\n                attention_heads_fused = attention.mean(dim=1)\n            else:\n                raise ValueError(\"head_fusion must be 'mean'\")\n            attention_heads_fused = attention_heads_fused + torch.eye(attention_heads_fused.size(-1))\n            attention_heads_fused = attention_heads_fused / attention_heads_fused.sum(dim=-1, keepdim=True)\n            result = torch.matmul(attention_heads_fused, result)\n        mask = result[0, 1:]\n        n_tokens = mask.shape[0]\n        side = int(n_tokens ** 0.5)\n        return mask[:side*side].reshape(side, side).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:16:46.667093Z","iopub.execute_input":"2025-05-26T22:16:46.667832Z","iopub.status.idle":"2025-05-26T22:16:46.674257Z","shell.execute_reply.started":"2025-05-26T22:16:46.667808Z","shell.execute_reply":"2025-05-26T22:16:46.673442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_vit = model_vit.to(device)  # ⚠️ Important\n\n# Image -> Tensor\nimg = Image.open(\"/kaggle/input/data/images_004/images/00006904_007.png\").convert('RGB')\na = transform(img)\nsample_image_input = a.unsqueeze(0).to(device)  # 👈 sur le même device que le modèle\n\n# Prédiction\nwith torch.no_grad():\n    output = model_vit(sample_image_input)\n    prob = torch.sigmoid(output).item()\n\n# Attention rollout\nrollout = VitAttentionRollout(model_vit)  # s'assurer qu'il est aussi sur le bon device\nattention_mask = rollout(sample_image_input)\n\n# Visualisation\nplt.imshow(a.permute(1, 2, 0).cpu().numpy().clip(0, 1))\nplt.imshow(attention_mask, cmap='jet', alpha=0.5, extent=(0, 224, 224, 0))\nplt.title(f\"Attention Rollout - ViT\\nProbabilité d'Atelectasis : {prob:.2f}\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:19:25.604561Z","iopub.execute_input":"2025-05-26T22:19:25.605059Z","iopub.status.idle":"2025-05-26T22:19:26.020617Z","shell.execute_reply.started":"2025-05-26T22:19:25.605037Z","shell.execute_reply":"2025-05-26T22:19:26.019954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom timm import create_model\nfrom scipy.ndimage import zoom\n\n# 1. Charger l'image et la transformer\nimg = Image.open(\"/kaggle/input/data/images_004/images/00006904_007.png\").convert('RGB')\na = transform(img)  # transform = Resize, ToTensor, Normalize, etc.\nsample_image_input = a.unsqueeze(0).to(device)\n\n# 2. Prédiction\nmodel_vit.eval()\nwith torch.no_grad():\n    output = model_vit(sample_image_input)\n    prob = torch.sigmoid(output).item()\n\n# 3. Attention Rollout\nrollout = VitAttentionRollout(model_vit)\nattention_mask = rollout(sample_image_input)  # shape: [14, 14] ou similaire\n\n# 4. Redimensionner la heatmap vers la taille de l’image\n# sample_image a la forme [C, H, W]\nh, w = a.shape[1], a.shape[2]\nzoom_factors = (h / attention_mask.shape[0], w / attention_mask.shape[1])\nattention_mask_resized = zoom(attention_mask, zoom_factors)\n\n# 5. Affichage image + heatmap lissée\nplt.imshow(a.permute(1, 2, 0).cpu().numpy().clip(0, 1))  # Image RGB\nplt.imshow(attention_mask_resized, cmap='jet', alpha=0.5, interpolation='bilinear')  # Heatmap\nplt.title(f\"Attention Rollout - ViT\\nProbabilité d'Atelectasis : {prob:.2f}\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:23:34.834019Z","iopub.execute_input":"2025-05-26T22:23:34.834705Z","iopub.status.idle":"2025-05-26T22:23:35.143488Z","shell.execute_reply.started":"2025-05-26T22:23:34.834681Z","shell.execute_reply":"2025-05-26T22:23:35.142827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi label","metadata":{}},{"cell_type":"code","source":"def supprimer_lignes_pathologies(df):\n    colonnes_exclues = [\n        'Hernia', 'Pneumonia', 'Fibrosis',\n        'Edema', 'Emphysema', 'Cardiomegaly',\n        'Pleural_Thickening'\n    ]\n    \n    # Supprimer les lignes qui ont au moins un 1 dans les colonnes concernées\n    df_filtré = df[~(df[colonnes_exclues].sum(axis=1) > 0)].reset_index(drop=True)\n    \n    return df_filtré\ndf = supprimer_lignes_pathologies(df)\ndf = df[df[\"View Position\"] == \"PA\"]\ndf.shape\n\nno_finding_df = df[df[\"No Finding\"] == 1]\n\n# 2. Sélectionner aléatoirement 8000 lignes parmi elles\nno_finding_sample = no_finding_df.sample(n=8000, random_state=42)\n\n# 3. Séparer le reste du DataFrame (tout sauf les lignes \"No Finding\" == 1)\nother_df = df[df[\"No Finding\"] != 1]\n\n# 4. Recombiner le tout\ndf = pd.concat([other_df, no_finding_sample], ignore_index=True)\n\ndf[\"Finding Labels\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:00:00.437918Z","iopub.execute_input":"2025-05-27T09:00:00.438751Z","iopub.status.idle":"2025-05-27T09:00:00.554956Z","shell.execute_reply.started":"2025-05-27T09:00:00.438699Z","shell.execute_reply":"2025-05-27T09:00:00.553810Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Finding Labels\nNo Finding                                                     8000\nInfiltration                                                   5270\nAtelectasis                                                    2210\nEffusion                                                       2086\nNodule                                                         1924\n                                                               ... \nNodule|Mass                                                       1\nAtelectasis|Mass|Nodule|Pneumothorax                              1\nAtelectasis|Consolidation|Effusion|Mass|Nodule|Pneumothorax       1\nMass|Nodule|Atelectasis                                           1\nEffusion|Nodule|Pneumothorax|Mass                                 1\nName: count, Length: 118, dtype: int64"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# 1. Dataset PyTorch multi-label\nclass ChestXrayDataset(Dataset):\n    def __init__(self, dataframe, label_cols, transform=None):\n        self.df = dataframe\n        self.label_cols = label_cols\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform = transform    \n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image Path']).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        labels = torch.tensor(row[self.label_cols].astype(np.float32).values, dtype=torch.float32)\n        return image, labels\n\n    def __afficher_image__(self, idx):\n        \"\"\"\n        Affiche l'image (dénormalisée) et ses labels associés.\n        \"\"\"\n        image, label = self[idx]\n\n        # # Dénormalisation\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        image = image * std + mean  # Inverser la normalisation\n\n        # Passage à la forme (H, W, C) pour matplotlib\n        image = image.permute(1, 2, 0).clamp(0, 1)\n\n        # Affichage\n        plt.imshow(image)\n        plt.title(f\"Label: {label}\")\n        plt.axis('off')\n        plt.show()\n\n        # Affichage des maladies détectées\n        nb_maladie = 0\n        for i in range(len(label)):\n            if label[i] == 1:\n                print(\"Maladies présentes :\")\n                print(f\" - {self.label_cols[i]}\")\n                nb_maladie += 1\n        if(nb_maladie == 0):\n            print(\"Aucune maladie\")\n\n# 3. Transforms\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.RandomHorizontalFlip(p=0.5),\n#     transforms.RandomRotation(10),\n#     transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n#     transforms.ColorJitter(brightness=0.2, contrast=0.2),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n# ])\n\n\ntransform = transforms.Compose([\n    LungROICrop(),\n    CLAHETransform(),\n    transforms.Resize((256, 256)),  # Agrandit\n    CenterCropZoom(zoom_factor=1.2),  # Zoom centré\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.RandomRotation(5),\n    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n])\n\n# 4. Split des données\n# 2. Préparation des données\nlabel_columns = ['Atelectasis', 'Infiltration', 'Effusion',  'Mass',\n    'Nodule', 'Pneumothorax', 'Consolidation']\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n\ntrain_dataset = ChestXrayDataset(train_df, label_columns, transform)\nval_dataset = ChestXrayDataset(val_df, label_columns, transform)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\n# 5. Modèle ViT\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(label_columns))\nmodel = model.cuda()\n\n\ncounts_pos = df[label_columns].sum()\ncounts_neg = df.shape[0] - counts_pos\n\npos_weight_tensor = torch.tensor((counts_neg / counts_pos).values, dtype=torch.float32).cuda()\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nbest_f1 = 0\ntrain_losses = []\nval_losses = []\n\n# 7. Validation\n\ndef evaluate(model, loader):\n    model.eval()\n    all_labels, all_outputs = [], []\n    total_loss = 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.cuda(), labels.cuda()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            outputs = torch.sigmoid(outputs).cpu().numpy()\n            all_outputs.extend(outputs)\n            all_labels.extend(labels.cpu().numpy())\n    val_loss = total_loss / len(loader)\n    auc = roc_auc_score(all_labels, all_outputs)\n    preds = (np.array(all_outputs) >= 0.5).astype(int)\n\n    f1 = f1_score(all_labels, preds)\n    precision = precision_score(all_labels, preds)\n    recall = recall_score(all_labels, preds)\n    accuracy = accuracy_score(all_labels, preds)\n    return val_loss, auc, f1, precision, recall, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:00:15.140719Z","iopub.execute_input":"2025-05-27T09:00:15.141039Z","iopub.status.idle":"2025-05-27T09:00:16.874440Z","shell.execute_reply.started":"2025-05-27T09:00:15.141005Z","shell.execute_reply":"2025-05-27T09:00:16.873691Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# 8. Boucle d'entraînement\npatience = 5\nbest_f1 = 0\nepochs_no_improve = 0\nmax_epochs = 50\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(max_epochs):\n    model.train()\n    epoch_loss = 0\n    start_time = time.time()\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        images, labels = images.cuda(), labels.cuda()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n    train_losses.append(epoch_loss)\n\n    val_loss, auc, f1, precision, recall, accuracy = evaluate(model, val_loader)\n    val_losses.append(val_loss)\n    elapsed = time.time() - start_time\n\n    print(f\"Epoch {epoch+1} - Time: {elapsed:.2f}s - Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - AUC: {auc:.4f} - F1: {f1:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Accuracy: {accuracy:.4f}\")\n\n    if f1 > best_f1:\n        best_f1 = f1\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n        with open(\"best_metrics.txt\", \"w\") as f:\n            f.write(f\"Epoch: {epoch+1}\\nLoss: {epoch_loss:.4f}\\nVal Loss: {val_loss:.4f}\\nAUC: {auc:.4f}\\nF1: {f1:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nAccuracy: {accuracy:.4f}\\n\")\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(\"Early stopping activé après\", patience, \"époques sans amélioration.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:00:17.472356Z","iopub.execute_input":"2025-05-27T09:00:17.472656Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 715/715 [22:30<00:00,  1.89s/it]\n","output_type":"stream"}],"execution_count":null}]}